{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032eaea-4bfa-4028-bba4-9500611877cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json\n",
    "import regex as re\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import math\n",
    "import pandas as pd\n",
    "import string\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "import argparse\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "import json \n",
    "from glob import glob \n",
    "import re \n",
    "from nltk import word_tokenize as lib_tokenizer \n",
    "import string\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, OkapiBM25Model\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "import numpy as np\n",
    "pandarallel.initialize(progress_bar=True, use_memory_fs=False, nb_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2907e-93f5-49b6-b03e-5585acc8e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk(query, topk = 100):\n",
    "    tokenized_query = query.split()\n",
    "    tfidf_query = tfidf_model[dictionary.doc2bow(tokenized_query)]\n",
    "    scores = bm25_index[tfidf_query]\n",
    "    top_n = np.argsort(scores)[::-1][:topk]\n",
    "    titles = [df_wiki.title.values[i] for i in top_n]\n",
    "    texts = [df_wiki.text.values[i] for i in top_n]\n",
    "    # print(titles)\n",
    "    # print(tfidf_query, scores)\n",
    "    return titles, texts, scores[top_n]\n",
    "\n",
    "def post_process(x):\n",
    "    x = \" \".join(word_tokenize(strip_context(x))).strip()\n",
    "    x = x.replace(\"\\n\",\" \")\n",
    "    x = \"\".join([i for i in x if i not in string.punctuation])\n",
    "    return x\n",
    "\n",
    "dict_map = dict({})  \n",
    "def word_tokenize(text): \n",
    "    global dict_map \n",
    "    words = text.split() \n",
    "    words_norm = [] \n",
    "    for w in words: \n",
    "        if dict_map.get(w, None) is None: \n",
    "            dict_map[w] = ' '.join(lib_tokenizer(w)).replace('``', '\"').replace(\"''\", '\"') \n",
    "        words_norm.append(dict_map[w]) \n",
    "    return words_norm \n",
    " \n",
    "def strip_answer_string(text): \n",
    "    text = text.strip() \n",
    "    while text[-1] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`': \n",
    "        if text[0] != '(' and text[-1] == ')' and '(' in text: \n",
    "            break \n",
    "        if text[-1] == '\"' and text[0] != '\"' and text.count('\"') > 1: \n",
    "            break \n",
    "        text = text[:-1].strip() \n",
    "    while text[0] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`': \n",
    "        if text[0] == '\"' and text[-1] != '\"' and text.count('\"') > 1: \n",
    "            break \n",
    "        text = text[1:].strip() \n",
    "    text = text.strip() \n",
    "    return text \n",
    " \n",
    "def strip_context(text): \n",
    "    text = text.replace('\\n', ' ') \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = text.strip() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65f394-31db-4da5-98f8-d937ff1c138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_cleaned_path = \"./processed/wikipedia_20220620_cleaned_v2.csv\"\n",
    "test_data_path =  \"./za-data/zac2022_testa_sample_submission.json\"\n",
    "topk = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb5933-b68e-4d75-b166-e60245125e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(wiki_cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60da89-8e60-4a7c-9b83-f38e70c009cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = df_wiki.fillna(\"NaN\")\n",
    "if \"title\" not in df_wiki.columns:\n",
    "    df_wiki[\"title\"] = df_wiki[\"titles=\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30b687-eff8-4de5-bff4-680b1289475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['bm25_text'] = df_wiki['bm25_text'].apply(lambda x: x.lower()).parallel_apply(post_process)\n",
    "corpus = [x.split() for x in df_wiki['bm25_text'].values]\n",
    "dictionary = Dictionary(corpus)\n",
    "bm25_model = OkapiBM25Model(dictionary=dictionary)\n",
    "bm25_corpus = bm25_model[list(map(dictionary.doc2bow, corpus))]\n",
    "bm25_index = SparseMatrixSimilarity(bm25_corpus, num_docs=len(corpus), num_terms=len(dictionary),normalize_queries=False, normalize_documents=False)\n",
    "tfidf_model = TfidfModel(dictionary=dictionary, smartirs='bnn')  # Enforce binary weighting of queries\n",
    "dictionary.save(\"./outputs/bm25_stage1/dict\")\n",
    "tfidf_model.save(\"./outputs/bm25_stage1/tfidf\")\n",
    "bm25_index.save(\"./outputs/bm25_stage1/bm25_index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
