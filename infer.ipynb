{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bm25_utils import BM25Gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "AUTH_TOKEN = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Load data & model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_PATH = \"./processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windows = pd.read_csv(f\"{PROCESSED_PATH}/corpus_clean.csv\")\n",
    "df_windows = df_windows.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Retriever **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 ranking\n",
    "bm25_model = BM25Gensim(f\"{PROCESSED_PATH}/outputs/bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from text_utils import preprocess\n",
    "\n",
    "model_name = 'nguyenvulebinh/vi-mrc-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=AUTH_TOKEN)\n",
    "model = AutoModel.from_pretrained(model_name, token=AUTH_TOKEN)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def embed_passage(passages, tokenizer, model, device='cpu'):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(passages, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    model = model.to(device)\n",
    "    encoded_input.to(device)\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    passage_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    return passage_embeddings\n",
    "\n",
    "def similarity_score(question, all_passage, tokenizer, model, device='cpu'):\n",
    "    process_paragraphs = [preprocess(doc) for doc in all_passage]\n",
    "    passage_embeddings = embed_passage(process_paragraphs, tokenizer, model, device)\n",
    "\n",
    "    question_embedding = embed_passage([question], tokenizer, model, device)\n",
    "\n",
    "    cos_scores = util.cos_sim(question_embedding, passage_embeddings)[0]\n",
    "    return cos_scores.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "titles = list(set([x for x in df_windows['title'].values]))\n",
    "tokenized_titles = [preprocess(x).lower().split() for x in titles]\n",
    "bm25_title = BM25Okapi(tokenized_titles)\n",
    "title_indices = list(range(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rau tiền đạo', 15.791215718696368],\n",
       " ['Tăng huyết áp thai kỳ', 13.2726289616419],\n",
       " ['Đái tháo đường thai kỳ', 13.2726289616419],\n",
       " ['Mang thai ngoài tử cung', 12.532570687265371],\n",
       " ['Rau bám mép có nguy hiểm không', 12.308922590612458]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hương đang mang thai và lo lắng mình có thể gặp phải rau tiền đạo. Hương có thể kiểm tra phát hiện bệnh này từ tuần thứ mấy của thai kỳ?\"\n",
    "# bm25_title.get_top_n(tokenized_question, titles, n=5)\n",
    "\n",
    "def get_topk_titles_with_score(question):\n",
    "  tokenized_question = preprocess(question).lower().split()\n",
    "  indices = bm25_title.get_top_n(tokenized_question, title_indices, n=5)\n",
    "  scores = bm25_title.get_batch_scores(tokenized_question, indices)\n",
    "  results = [[titles[i], s] for i, s in zip(indices, scores)]\n",
    "  return results\n",
    "\n",
    "get_topk_titles_with_score(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "similarity_score_shorted = partial(similarity_score, tokenizer=tokenizer, model=model, device=device)\n",
    "\n",
    "def get_corpus(question):\n",
    "  #Bm25 retrieval for top200 candidates\n",
    "  query = preprocess(question).lower()\n",
    "  top_n, bm25_scores = bm25_model.get_topk(query, topk=500)\n",
    "  titles_with_scores = get_topk_titles_with_score(query)\n",
    "  score_map = {k: v for k, v in titles_with_scores}\n",
    "\n",
    "  filtered_indices = [i for i, v in enumerate(top_n) if str(df_windows.title.values[v]) in score_map]\n",
    "  top_n, bm25_scores = top_n[filtered_indices], bm25_scores[filtered_indices]\n",
    "  # print(score_map)\n",
    "  \n",
    "  titles = [preprocess(df_windows.title.values[i]) for i in top_n]\n",
    "  texts = [preprocess(df_windows.text.values[i]).lower() for i in top_n]\n",
    "  \n",
    "  # Reranking with for top10\n",
    "  # question = preprocess(question)\n",
    "  ranking_texts = similarity_score_shorted(query, texts)\n",
    "  ranking_titles = np.array([score_map[s] for s in titles])\n",
    "  ranking_scores = ranking_texts*ranking_texts * bm25_scores * ranking_titles\n",
    "\n",
    "  best_idxs = np.argsort(ranking_scores)[-10:]\n",
    "  ranking_scores = np.array(ranking_scores)[best_idxs]\n",
    "  texts = np.array(texts)[best_idxs]\n",
    "  titles = np.array(titles)[best_idxs]\n",
    "\n",
    "  return texts, ranking_scores\n",
    "\n",
    "# get_corpus(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 3. Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "id2label = {\n",
    "    0: 'False',\n",
    "    1: 'True',\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    'False': 0,\n",
    "    'True': 1,\n",
    "}\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "class QAEnsembleModel(nn.Module):\n",
    "    def __init__(self, model_checkpoint, device=\"cpu\"):\n",
    "        super(QAEnsembleModel, self).__init__()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            num_labels=num_labels,\n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            use_fast=True,\n",
    "            cls_token=\"<s>\",\n",
    "            sep_token=\"</s>\"\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_checkpoint,\n",
    "            num_labels=num_labels\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, question, choices, texts, ranking_scores=None):\n",
    "        if ranking_scores is None:\n",
    "            ranking_scores = np.ones((len(texts),))\n",
    "\n",
    "        best_positive_index = 0\n",
    "        best_positive_score = 0\n",
    "        all_choices_answers = [\"0\"]*len(choices)\n",
    "        for idx, c in enumerate(choices):\n",
    "            if c.split('.', 1)[0] in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
    "                c = c[2:].strip()\n",
    "\n",
    "            # answers = []\n",
    "            # answer_scores = []\n",
    "            for text, score in zip(texts, ranking_scores):\n",
    "                prompt = f\"<s>{text}</s>{question}</s>{c}</s>\"\n",
    "                model_inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    # padding=\"max_length\",\n",
    "                    # max_length=256,\n",
    "                    # truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                outputs = self.model(**model_inputs)\n",
    "                prediction = torch.argmax(outputs[0], axis=1).item()\n",
    "                _l = outputs[0].detach().numpy()[0] * score\n",
    "                # {0,1}, score\n",
    "                # answers.append(str(prediction))\n",
    "                # answer_scores.append(_l[prediction])\n",
    "                confident_score = _l[1] - _l[0]\n",
    "                if confident_score > best_positive_score:\n",
    "                    best_positive_score = confident_score\n",
    "                    best_positive_index = idx\n",
    "                # prioritize positive answers\n",
    "                if prediction == 1:\n",
    "                    all_choices_answers[idx] = str(prediction)\n",
    "                    break\n",
    "            # find best choices\n",
    "            # best_answers_idx = np.argmax(np.array(answer_scores))\n",
    "            # choice_answer = answers[best_answers_idx]\n",
    "            # all_choices_answers.append(choice_answer)\n",
    "        \n",
    "        # do some trick to correct answer, each question have atleast one correct choice :)))\n",
    "        if '1' not in all_choices_answers:\n",
    "            all_choices_answers[best_positive_index] = \"1\"\n",
    "        answer = \"\".join(all_choices_answers)\n",
    "\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qa_model import QAEnsembleModel\n",
    "\n",
    "qa_model = QAEnsembleModel(\"./model/fine_tuned_model/chieunq/xlm-r-base-uit-viquad/\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0100\n"
     ]
    }
   ],
   "source": [
    "texts, ranking_scores = get_corpus(question)\n",
    "choices = [\"A. Tuần 10\", \"B.Tuần 20\", \"C. Tuần 30\", \"D. Tuần 40\" ]\n",
    "answer = qa_model(question, choices, texts, ranking_scores=ranking_scores)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalapa-mcqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
